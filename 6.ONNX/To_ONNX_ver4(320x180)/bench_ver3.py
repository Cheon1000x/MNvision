#!/usr/bin/env python3
"""
PT ëª¨ë¸ vs ONNX ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
ì •í™•ë„, ì†ë„, íƒì§€ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµ
"""

import cv2
import numpy as np
import onnxruntime as ort
import time
import os
from ultralytics import YOLO
from pathlib import Path
import matplotlib.pyplot as plt
import pandas as pd

class PTModel:
    """ì›ë³¸ PyTorch ëª¨ë¸ ë˜í¼"""
    def __init__(self, model_path, conf_threshold=0.3):
        self.model = YOLO(model_path)
        self.conf_threshold = conf_threshold
        self.class_names = list(self.model.names.values())
        
    def predict(self, image_path):
        start_time = time.perf_counter()
        results = self.model(image_path, conf=self.conf_threshold, verbose=False)[0]
        inference_time = (time.perf_counter() - start_time) * 1000
        
        boxes = []
        scores = []
        labels = []
        
        if results.boxes is not None and len(results.boxes) > 0:
            for box in results.boxes:
                conf = box.conf.item()
                cls_id = int(box.cls.item())
                xyxy = box.xyxy[0].tolist()
                
                boxes.append(xyxy)
                scores.append(conf)
                labels.append(cls_id)
        
        return np.array(boxes), np.array(scores), np.array(labels), inference_time

class ONNXModel:
    """ONNX ëª¨ë¸ ë˜í¼ (ê¸°ì¡´ ONNXDetector ê¸°ë°˜)"""
    def __init__(self, model_path, input_size=(320, 192), conf_threshold=0.3, iou_threshold=0.1):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        self.input_width, self.input_height = input_size
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # ìˆ˜ì •ëœ í´ë˜ìŠ¤ ìˆœì„œ (í˜„ì¬ ë²„ì „)
        self.class_names = [
            'forklift-right',       # 0
            'forklift-left',        # 1  
            'forklift-horizontal',  # 2
            'person',               # 3
            'forklift-vertical',    # 4
            'object',               # 5
            ''                      # 6
        ]

    def preprocess(self, image):
        original_h, original_w = image.shape[:2]
        
        # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë¦¬ì‚¬ì´ì¦ˆ
        scale = min(self.input_width / original_w, self.input_height / original_h)
        new_w, new_h = int(original_w * scale), int(original_h * scale)
        resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)

        # íŒ¨ë”© ì¶”ê°€
        top_pad = (self.input_height - new_h) // 2
        bottom_pad = self.input_height - new_h - top_pad
        left_pad = (self.input_width - new_w) // 2
        right_pad = self.input_width - new_w - left_pad

        padded_image = cv2.copyMakeBorder(
            resized_image, top_pad, bottom_pad, left_pad, right_pad,
            cv2.BORDER_CONSTANT, value=(114, 114, 114)
        )

        # ì •ê·œí™” ë° ì°¨ì› ë³€í™˜
        input_tensor = padded_image.astype(np.float32) / 255.0
        input_tensor = np.transpose(input_tensor, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, axis=0)

        return input_tensor, scale, (top_pad, left_pad)

    def postprocess(self, output, original_width, original_height, scale, padding):
        pred = output.squeeze(0)
        
        # YOLO ì¶œë ¥ íŒŒì‹±
        boxes_raw = pred[0:4, :].T
        objectness_raw = pred[4, :]
        class_scores_raw = pred[5:11, :].T

        # ì‹œê·¸ëª¨ì´ë“œ ì ìš©
        objectness = self.sigmoid(objectness_raw)
        class_scores = self.sigmoid(class_scores_raw)

        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        scores = objectness[:, np.newaxis] * class_scores
        scores_max = np.max(scores, axis=1)
        labels = np.argmax(scores, axis=1)

        # ì„ê³„ê°’ í•„í„°ë§
        keep_mask = scores_max > self.conf_threshold
        if keep_mask.sum() == 0:
            return np.array([]), np.array([]), np.array([])

        boxes_filtered = boxes_raw[keep_mask]
        scores_filtered = scores_max[keep_mask]
        labels_filtered = labels[keep_mask]

        # ì¤‘ì‹¬ì  â†’ ì¢Œìƒë‹¨/ìš°í•˜ë‹¨ ë³€í™˜
        boxes_xyxy = np.copy(boxes_filtered)
        boxes_xyxy[:, 0] = boxes_filtered[:, 0] - boxes_filtered[:, 2] / 2
        boxes_xyxy[:, 1] = boxes_filtered[:, 1] - boxes_filtered[:, 3] / 2
        boxes_xyxy[:, 2] = boxes_filtered[:, 0] + boxes_filtered[:, 2] / 2
        boxes_xyxy[:, 3] = boxes_filtered[:, 1] + boxes_filtered[:, 3] / 2
        
        # NMS
        boxes_for_nms = np.array([[x1, y1, x2-x1, y2-y1] for x1, y1, x2, y2 in boxes_xyxy])
        indices = cv2.dnn.NMSBoxes(
            boxes_for_nms.tolist(), scores_filtered.tolist(), 
            self.conf_threshold, self.iou_threshold
        )
        
        if len(indices) == 0:
            return np.array([]), np.array([]), np.array([])
        
        indices = indices.flatten()
        boxes_final = boxes_xyxy[indices]
        scores_final = scores_filtered[indices]
        labels_final = labels_filtered[indices]

        # íŒ¨ë”© ë° ìŠ¤ì¼€ì¼ë§ ì—­ë³€í™˜
        top_pad, left_pad = padding
        boxes_final[:, 0] -= left_pad
        boxes_final[:, 1] -= top_pad
        boxes_final[:, 2] -= left_pad
        boxes_final[:, 3] -= top_pad
        boxes_final /= scale

        # í´ë¦¬í•‘
        boxes_final[:, 0] = np.clip(boxes_final[:, 0], 0, original_width)
        boxes_final[:, 1] = np.clip(boxes_final[:, 1], 0, original_height)
        boxes_final[:, 2] = np.clip(boxes_final[:, 2], 0, original_width)
        boxes_final[:, 3] = np.clip(boxes_final[:, 3], 0, original_height)

        return boxes_final, scores_final, labels_final

    def sigmoid(self, x):
        x = np.clip(x, -500, 500)
        return 1 / (1 + np.exp(-x))

    def predict(self, image_path):
        # ì´ë¯¸ì§€ ë¡œë“œ
        original_image_bgr = cv2.imread(image_path)
        original_image = cv2.cvtColor(original_image_bgr, cv2.COLOR_BGR2RGB)
        original_height, original_width = original_image.shape[:2]

        # ì „ì²˜ë¦¬
        input_tensor, scale, padding = self.preprocess(original_image)

        # ONNX ì¶”ë¡ 
        start_time = time.perf_counter()
        outputs = self.session.run([self.output_name], {self.input_name: input_tensor})
        inference_time = (time.perf_counter() - start_time) * 1000
        
        # í›„ì²˜ë¦¬
        boxes, scores, labels = self.postprocess(
            outputs[0], original_width, original_height, scale, padding
        )
        
        return boxes, scores, labels, inference_time

class BenchmarkComparator:
    """PTì™€ ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë¹„êµê¸°"""
    
    def __init__(self, pt_model_path, onnx_model_path, conf_threshold=0.3):
        print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.pt_model = PTModel(pt_model_path, conf_threshold)
        self.onnx_model = ONNXModel(onnx_model_path, conf_threshold=conf_threshold)
        
        print(f"âœ… PT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.pt_model.class_names)}ê°œ í´ë˜ìŠ¤")
        print(f"âœ… ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.onnx_model.class_names)}ê°œ í´ë˜ìŠ¤")
        
    def calculate_iou(self, box1, box2):
        """ë‘ ë°•ìŠ¤ ê°„ì˜ IoU ê³„ì‚°"""
        x1_max = max(box1[0], box2[0])
        y1_max = max(box1[1], box2[1])
        x2_min = min(box1[2], box2[2])
        y2_min = min(box1[3], box2[3])
        
        if x2_min <= x1_max or y2_min <= y1_max:
            return 0.0
        
        intersection = (x2_min - x1_max) * (y2_min - y1_max)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def match_detections(self, pt_boxes, pt_labels, onnx_boxes, onnx_labels, iou_threshold=0.5):
        """PTì™€ ONNX íƒì§€ ê²°ê³¼ ë§¤ì¹­"""
        matches = []
        unmatched_pt = list(range(len(pt_boxes)))
        unmatched_onnx = list(range(len(onnx_boxes)))
        
        for i, pt_box in enumerate(pt_boxes):
            best_iou = 0
            best_match = -1
            
            for j, onnx_box in enumerate(onnx_boxes):
                if j in unmatched_onnx:
                    iou = self.calculate_iou(pt_box, onnx_box)
                    if iou > best_iou and iou > iou_threshold:
                        best_iou = iou
                        best_match = j
            
            if best_match != -1:
                matches.append({
                    'pt_idx': i,
                    'onnx_idx': best_match,
                    'iou': best_iou,
                    'class_match': pt_labels[i] == onnx_labels[best_match]
                })
                unmatched_pt.remove(i)
                unmatched_onnx.remove(best_match)
        
        return matches, unmatched_pt, unmatched_onnx

    def benchmark_single_image(self, image_path):
        """ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸ“¸ ì´ë¯¸ì§€ ì²˜ë¦¬: {os.path.basename(image_path)}")
        
        # PT ëª¨ë¸ ì˜ˆì¸¡
        pt_boxes, pt_scores, pt_labels, pt_time = self.pt_model.predict(image_path)
        
        # ONNX ëª¨ë¸ ì˜ˆì¸¡
        onnx_boxes, onnx_scores, onnx_labels, onnx_time = self.onnx_model.predict(image_path)
        
        # ê²°ê³¼ ë§¤ì¹­
        matches, unmatched_pt, unmatched_onnx = self.match_detections(
            pt_boxes, pt_labels, onnx_boxes, onnx_labels
        )
        
        # í†µê³„ ê³„ì‚°
        total_pt = len(pt_boxes)
        total_onnx = len(onnx_boxes)
        matched_count = len(matches)
        class_match_count = sum(1 for m in matches if m['class_match'])
        
        results = {
            'image': os.path.basename(image_path),
            'pt_detections': total_pt,
            'onnx_detections': total_onnx,
            'matched_detections': matched_count,
            'class_accuracy': class_match_count / matched_count if matched_count > 0 else 0,
            'pt_inference_time': pt_time,
            'onnx_inference_time': onnx_time,
            'speed_improvement': pt_time / onnx_time if onnx_time > 0 else 0,
            'unmatched_pt': len(unmatched_pt),
            'unmatched_onnx': len(unmatched_onnx)
        }
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        print(f"   PT íƒì§€: {total_pt}ê°œ, ONNX íƒì§€: {total_onnx}ê°œ")
        print(f"   ë§¤ì¹­: {matched_count}ê°œ, í´ë˜ìŠ¤ ì •í™•ë„: {results['class_accuracy']:.1%}")
        print(f"   ì†ë„: PT {pt_time:.1f}ms vs ONNX {onnx_time:.1f}ms ({results['speed_improvement']:.1f}x)")
        
        return results

    def benchmark_multiple_images(self, image_paths):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¢…í•© ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸš€ ë‹¤ì¤‘ ì´ë¯¸ì§€ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 60)
        
        all_results = []
        
        for image_path in image_paths:
            if os.path.exists(image_path):
                result = self.benchmark_single_image(image_path)
                all_results.append(result)
            else:
                print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
        
        if not all_results:
            print("âŒ ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì¢…í•© í†µê³„
        self.print_summary_stats(all_results)
        
        # ê²°ê³¼ ì‹œê°í™”
        self.visualize_results(all_results)
        
        return all_results

    def print_summary_stats(self, results):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ í†µê³„ ì¶œë ¥"""
        df = pd.DataFrame(results)
        
        print(f"\nğŸ“Š ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ({len(results)}ê°œ ì´ë¯¸ì§€)")
        print("=" * 60)
        
        print(f"ğŸ¯ íƒì§€ ì„±ëŠ¥:")
        print(f"   í‰ê·  PT íƒì§€: {df['pt_detections'].mean():.1f}ê°œ")
        print(f"   í‰ê·  ONNX íƒì§€: {df['onnx_detections'].mean():.1f}ê°œ")
        print(f"   í‰ê·  ë§¤ì¹­ë¥ : {(df['matched_detections'] / df['pt_detections']).mean():.1%}")
        print(f"   í‰ê·  í´ë˜ìŠ¤ ì •í™•ë„: {df['class_accuracy'].mean():.1%}")
        
        print(f"\nâš¡ ì†ë„ ì„±ëŠ¥:")
        print(f"   í‰ê·  PT ì‹œê°„: {df['pt_inference_time'].mean():.1f}ms")
        print(f"   í‰ê·  ONNX ì‹œê°„: {df['onnx_inference_time'].mean():.1f}ms")
        print(f"   í‰ê·  ì†ë„ í–¥ìƒ: {df['speed_improvement'].mean():.1f}x")
        
        print(f"\nğŸ“ˆ ìƒì„¸ í†µê³„:")
        print(f"   ì´ PT íƒì§€: {df['pt_detections'].sum()}ê°œ")
        print(f"   ì´ ONNX íƒì§€: {df['onnx_detections'].sum()}ê°œ")
        print(f"   ì´ ë§¤ì¹­: {df['matched_detections'].sum()}ê°œ")
        print(f"   ë¯¸ë§¤ì¹­ PT: {df['unmatched_pt'].sum()}ê°œ")
        print(f"   ë¯¸ë§¤ì¹­ ONNX: {df['unmatched_onnx'].sum()}ê°œ")

    def visualize_results(self, results):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‹œê°í™”"""
        df = pd.DataFrame(results)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('PT vs ONNX ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
        
        # 1. íƒì§€ ìˆ˜ ë¹„êµ
        axes[0, 0].bar(['PT', 'ONNX'], [df['pt_detections'].sum(), df['onnx_detections'].sum()])
        axes[0, 0].set_title('ì´ íƒì§€ ìˆ˜ ë¹„êµ')
        axes[0, 0].set_ylabel('íƒì§€ ìˆ˜')
        
        # 2. ì¶”ë¡  ì‹œê°„ ë¹„êµ
        x = range(len(results))
        axes[0, 1].plot(x, df['pt_inference_time'], 'o-', label='PT', linewidth=2)
        axes[0, 1].plot(x, df['onnx_inference_time'], 's-', label='ONNX', linewidth=2)
        axes[0, 1].set_title('ì´ë¯¸ì§€ë³„ ì¶”ë¡  ì‹œê°„')
        axes[0, 1].set_xlabel('ì´ë¯¸ì§€ ì¸ë±ìŠ¤')
        axes[0, 1].set_ylabel('ì‹œê°„ (ms)')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. í´ë˜ìŠ¤ ì •í™•ë„
        axes[1, 0].bar(range(len(results)), df['class_accuracy'])
        axes[1, 0].set_title('ì´ë¯¸ì§€ë³„ í´ë˜ìŠ¤ ì •í™•ë„')
        axes[1, 0].set_xlabel('ì´ë¯¸ì§€ ì¸ë±ìŠ¤')
        axes[1, 0].set_ylabel('ì •í™•ë„')
        axes[1, 0].set_ylim(0, 1)
        
        # 4. ì†ë„ í–¥ìƒ ë°°ìˆ˜
        axes[1, 1].bar(range(len(results)), df['speed_improvement'])
        axes[1, 1].set_title('ì†ë„ í–¥ìƒ ë°°ìˆ˜')
        axes[1, 1].set_xlabel('ì´ë¯¸ì§€ ì¸ë±ìŠ¤')
        axes[1, 1].set_ylabel('ë°°ìˆ˜ (x)')
        axes[1, 1].axhline(y=1, color='r', linestyle='--', alpha=0.7, label='1x (ë™ì¼)')
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')
        print(f"\nğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥: benchmark_results.png")
        plt.show()

def find_images_in_folder(folder_path):
    """í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë°˜í™˜"""
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
    image_files = []
    
    if not os.path.exists(folder_path):
        print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
        return []
    
    print(f"ğŸ“ í´ë” ìŠ¤ìº” ì¤‘: {folder_path}")
    
    # í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ê²€ìƒ‰ (í•˜ìœ„ í´ë” í¬í•¨)
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_ext = Path(file).suffix.lower()
            if file_ext in image_extensions:
                full_path = os.path.join(root, file)
                image_files.append(full_path)
    
    print(f"âœ… ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
    
    # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬
    image_files.sort()
    
    # ì²˜ìŒ 10ê°œ íŒŒì¼ëª… ì¶œë ¥
    if image_files:
        print("ğŸ“‹ ë°œê²¬ëœ ì´ë¯¸ì§€ (ì²˜ìŒ 10ê°œ):")
        for i, img_path in enumerate(image_files[:10]):
            rel_path = os.path.relpath(img_path, folder_path)
            print(f"   {i+1}. {rel_path}")
        if len(image_files) > 10:
            print(f"   ... ì™¸ {len(image_files) - 10}ê°œ")
    
    return image_files

def get_user_inputs():
    """ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ëŠ” í•¨ìˆ˜"""
    
    print("ğŸ”§ ë²¤ì¹˜ë§ˆí¬ ì„¤ì •")
    print("=" * 40)
    
    # PT ëª¨ë¸ ê²½ë¡œ
    print("\n1ï¸âƒ£ PT ëª¨ë¸ ê²½ë¡œ:")
    pt_default = r"C:\Users\KDT-13\Desktop\Group 6_\MNvision\6.ONNX\To_ONNX_ver5(320x180)\best.pt"
    pt_path = input(f"PT ëª¨ë¸ ê²½ë¡œ [{pt_default}]: ").strip()
    if not pt_path:
        pt_path = pt_default
    
    # ONNX ëª¨ë¸ ê²½ë¡œ  
    print("\n2ï¸âƒ£ ONNX ëª¨ë¸ ê²½ë¡œ:")
    onnx_default = r"C:\Users\KDT-13\Desktop\Group 6_\MNvision\6.ONNX\To_ONNX_ver5(320x180)\yolov8_custom_fixed.onnx"
    onnx_path = input(f"ONNX ëª¨ë¸ ê²½ë¡œ [{onnx_default}]: ").strip()
    if not onnx_path:
        onnx_path = onnx_default
    
    # ì´ë¯¸ì§€ í´ë” ê²½ë¡œ
    print("\n3ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë”:")
    print("í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼(.jpg, .png ë“±)ì´ ìë™ìœ¼ë¡œ ê²€ìƒ‰ë©ë‹ˆë‹¤.")
    folder_default = r"C:\Users\KDT-13\Desktop\Group 6_\MNvision\2.Data\photo_cam1"
    folder_path = input(f"ì´ë¯¸ì§€ í´ë” ê²½ë¡œ [{folder_default}]: ").strip()
    if not folder_path:
        folder_path = folder_default
    
    # ì‹ ë¢°ë„ ì„ê³„ê°’
    print("\n4ï¸âƒ£ ì‹ ë¢°ë„ ì„ê³„ê°’:")
    conf_default = "0.3"
    conf_input = input(f"ì‹ ë¢°ë„ ì„ê³„ê°’ (0.0-1.0) [{conf_default}]: ").strip()
    try:
        conf_threshold = float(conf_input) if conf_input else float(conf_default)
        if not 0.0 <= conf_threshold <= 1.0:
            raise ValueError
    except ValueError:
        print(f"âš ï¸ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ {conf_default} ì‚¬ìš©")
        conf_threshold = float(conf_default)
    
    # ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ
    print("\n5ï¸âƒ£ ìµœëŒ€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜:")
    max_default = "10"
    max_input = input(f"ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ (0=ì „ì²´) [{max_default}]: ").strip()
    try:
        max_images = int(max_input) if max_input else int(max_default)
        if max_images < 0:
            raise ValueError
    except ValueError:
        print(f"âš ï¸ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ {max_default} ì‚¬ìš©")
        max_images = int(max_default)
    
    return pt_path, onnx_path, folder_path, conf_threshold, max_images

def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    print("ğŸ”¬ PT vs ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ")
    print("=" * 60)
    
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    pt_path, onnx_path, folder_path, conf_threshold, max_images = get_user_inputs()
    
    print(f"\nğŸ“‹ ì„¤ì • í™•ì¸:")
    print(f"   PT ëª¨ë¸: {pt_path}")
    print(f"   ONNX ëª¨ë¸: {onnx_path}")
    print(f"   ì´ë¯¸ì§€ í´ë”: {folder_path}")
    print(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {conf_threshold}")
    print(f"   ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜: {max_images if max_images > 0 else 'ì „ì²´'}")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(pt_path):
        print(f"âŒ PT ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pt_path}")
        return
    
    if not os.path.exists(onnx_path):
        print(f"âŒ ONNX ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {onnx_path}")
        return
    
    # ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰
    image_files = find_images_in_folder(folder_path)
    
    if not image_files:
        print("âŒ í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì´ë¯¸ì§€ ìˆ˜ ì œí•œ
    if max_images > 0 and len(image_files) > max_images:
        image_files = image_files[:max_images]
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ {max_images}ê°œë¡œ ì œí•œí•©ë‹ˆë‹¤.")
    
    # ì‹¤í–‰ í™•ì¸
    print(f"\nğŸš€ {len(image_files)}ê°œ ì´ë¯¸ì§€ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n) [y]: ").strip().lower()
    if confirm and confirm != 'y':
        print("âŒ ë²¤ì¹˜ë§ˆí¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "=" * 60)
    
    # ë²¤ì¹˜ë§ˆí¬ ê°ì²´ ìƒì„±
    comparator = BenchmarkComparator(pt_path, onnx_path, conf_threshold)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = comparator.benchmark_multiple_images(image_files)
    
    print(f"\nğŸ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("ê²°ê³¼ íŒŒì¼:")
    print("- benchmark_results.png: ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„")
    print(f"- ì´ ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")

if __name__ == "__main__":
    main()