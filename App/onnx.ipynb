{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e51b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ONNX 모델 변환 시작 ---\n",
      "Ultralytics 8.3.145  Python-3.9.21 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "YOLOv8s-seg summary (fused): 85 layers, 11,810,560 parameters, 0 gradients, 42.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'resources\\models\\yolov8s-seg.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 116, 8400), (1, 32, 160, 160)) (22.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 20...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.8s, saved as 'resources\\models\\yolov8s-seg.onnx' (45.3 MB)\n",
      "\n",
      "Export complete (2.5s)\n",
      "Results saved to \u001b[1mC:\\Users\\kdt\\OneDrive\\ \\PROJECT_MNV\\App\\resources\\models\u001b[0m\n",
      "Predict:         yolo predict task=segment model=resources\\models\\yolov8s-seg.onnx imgsz=640  \n",
      "Validate:        yolo val task=segment model=resources\\models\\yolov8s-seg.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "\n",
      "ONNX 모델 변환 성공!\n",
      "생성된 ONNX 모델을 로드하여 테스트해 보세요.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. 모델 로드 (학습된 .pt 모델 또는 사전 학습된 모델)\n",
    "# model = YOLO('path/to/your/best.pt') # 학습된 모델\n",
    "# model = YOLO(\"./resources/models/yolov8n-seg.pt\") # 사전 학습된 세그멘테이션 모델 예시\n",
    "# model = YOLO(\"./resources/models/yolo8n_seg_0528.pt\") # 사전 학습된 세그멘테이션 모델 예시\n",
    "# model.load(\"./resources/models/yolo8n_seg_0528.pt\")\n",
    "# model = YOLO(\"./resources/models/yolov8n-seg.pt\") # 사전 학습된 세그멘테이션 모델 예시\n",
    "model = YOLO(\"./resources/models/yolov8s-seg.pt\") # 사전 학습된 세그멘테이션 모델 예시\n",
    "# model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "# 2. ONNX로 변환 시도 (보수적인 설정)\n",
    "print(\"--- ONNX 모델 변환 시작 ---\")\n",
    "try:\n",
    "    model.export(\n",
    "        format='onnx',\n",
    "        imgsz=640,          # 학습 시 사용한 이미지 크기\n",
    "        opset=20,           # 호환성이 높은 opset 버전 선택 (13 또는 14 권장)\n",
    "        dynamic=False,      # 동적 차원 비활성화 (가장 단순한 형태)\n",
    "        half=False,         # FP16 양자화 비활성화\n",
    "        int8=False,         # INT8 양자화 비활성화\n",
    "        simplify=True,     # 그래프 최적화 비활성화 (InvalidProtobuf 방지)\n",
    "        name='./resources/models/test_dfnq.onnx', # 파일명 지정 (선택 사항)\n",
    "        verbose=True        # 변환 과정 상세 로그 출력\n",
    "    )\n",
    "    print(\"\\nONNX 모델 변환 성공!\")\n",
    "    print(\"생성된 ONNX 모델을 로드하여 테스트해 보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nONNX 모델 변환 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f522389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.145  Python-3.9.21 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "WARNING imgsz=[720, 1280] must be multiple of max stride 32, updating to [736, 1280]\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,260,209 parameters, 0 gradients, 12.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'resources\\models\\ver1_test.pt' with input shape (1, 3, 736, 1280) BCHW and output shape(s) ((1, 47, 19320), (1, 32, 184, 320)) (6.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 13...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.7s, saved as 'resources\\models\\ver1_test.onnx' (12.9 MB)\n",
      "\n",
      "Export complete (1.1s)\n",
      "Results saved to \u001b[1mC:\\Users\\kdt\\OneDrive\\ \\PROJECT_MNV\\App\\resources\\models\u001b[0m\n",
      "Predict:         yolo predict task=segment model=resources\\models\\ver1_test.onnx imgsz=736,1280  \n",
      "Validate:        yolo val task=segment model=resources\\models\\ver1_test.onnx imgsz=736,1280 data=C:\\Users\\K\\Desktop\\Group_6\\TEST3\\dataset.yaml  WARNING  non-PyTorch val requires square images, 'imgsz=[736, 1280]' will not work. Use export 'imgsz=1280' if val is required.\n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'resources\\\\models\\\\ver1_test.onnx'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"./resources/models/ver1_test.pt\")\n",
    "\n",
    "model.export(\n",
    "        format=\"onnx\",\n",
    "        imgsz=(720, 1280),\n",
    "                             # 학습 시 사용한 이미지 크기\n",
    "        opset=13,           # 호환성이 높은 opset 버전 선택 (13 또는 14 권장)\n",
    "        dynamic=False,      # 동적 차원 비활성화 (가장 단순한 형태)\n",
    "        half=False,         # FP16 양자화 비활성화\n",
    "        int8=False,         # INT8 양자화 비활성화\n",
    "        simplify=False,     # 그래프 최적화 비활성화 (InvalidProtobuf 방지)\n",
    "        name='./resources/models/test_dfnq.onnx', # 파일명 지정 (선택 사항)\n",
    "        verbose=True        # 변환 과정 상세 로그 출력\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab07c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.145  Python-3.9.21 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "WARNING half=True only compatible with GPU export, i.e. use device=0\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,260,209 parameters, 0 gradients, 12.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'resources\\models\\ver1_test.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 47, 8400), (1, 32, 160, 160)) (6.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 13...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.0s, saved as 'resources\\models\\ver1_test.onnx' (12.7 MB)\n",
      "\n",
      "Export complete (1.3s)\n",
      "Results saved to \u001b[1mC:\\Users\\kdt\\OneDrive\\ \\PROJECT_MNV\\App\\resources\\models\u001b[0m\n",
      "Predict:         yolo predict task=segment model=resources\\models\\ver1_test.onnx imgsz=640  \n",
      "Validate:        yolo val task=segment model=resources\\models\\ver1_test.onnx imgsz=640 data=C:\\Users\\K\\Desktop\\Group_6\\TEST3\\dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'resources\\\\models\\\\ver1_test.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"./resources/models/ver1_test.pt\")\n",
    "\n",
    "model.export(\n",
    "        format=\"onnx\",\n",
    "        # imgsz=(720, 1280),\n",
    "        imgsz=640,\n",
    "                             # 학습 시 사용한 이미지 크기\n",
    "        opset=13,           # 호환성이 높은 opset 버전 선택 (13 또는 14 권장)\n",
    "        dynamic=False,      # 동적 차원 비활성화 (가장 단순한 형태)\n",
    "        half=True,         # FP16 양자화 비활성화\n",
    "        # int8=True,         # INT8 양자화 비활성화\n",
    "        simplify=True,     # 그래프 최적화 비활성화 (InvalidProtobuf 방지)\n",
    "        name='./resources/models/test_dfnq.onnx', # 파일명 지정 (선택 사항)\n",
    "        verbose=True        # 변환 과정 상세 로그 출력\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c6df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim {\n",
      "  dim_value: 1\n",
      "}\n",
      "dim {\n",
      "  dim_value: 3\n",
      "}\n",
      "dim {\n",
      "  dim_value: 736\n",
      "}\n",
      "dim {\n",
      "  dim_value: 1280\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "model_path = \"./resources/models/ver1_test.onnx\"\n",
    "onnx_model = onnx.load(model_path)\n",
    "print(onnx_model.graph.input[0].type.tensor_type.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf2e2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_results(image, detections, proto, mask_threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    image: (H, W, 3) numpy image, dtype=float32, range 0~1\n",
    "    detections: (N, 6 + M) → [x1, y1, x2, y2, conf, cls, mask_coefs...]\n",
    "    proto: (mask_dim, mask_h, mask_w)\n",
    "    \"\"\"\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    mask_dim = proto.shape[0]\n",
    "    mask_h, mask_w = proto.shape[1:]\n",
    "\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det[:6]\n",
    "        mask_coefs = det[6:]\n",
    "\n",
    "        # 바운딩 박스 그리기\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "        label = f'{int(cls)} {conf:.2f}' if class_names is None else f'{class_names[int(cls)]} {conf:.2f}'\n",
    "        cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = (mask_coefs @ proto.reshape(mask_dim, -1)).reshape(mask_h, mask_w)\n",
    "        mask = 1 / (1 + np.exp(-mask))  # sigmoid\n",
    "        mask = cv2.resize(mask, (img_w, img_h))  # 원본 이미지 크기로\n",
    "        mask = (mask > mask_threshold).astype(np.uint8)\n",
    "\n",
    "        # 마스크 색깔 입히기\n",
    "        colored_mask = np.zeros_like(image, dtype=np.uint8)\n",
    "        color = np.random.randint(0, 255, size=3).tolist()\n",
    "        for c in range(3):\n",
    "            colored_mask[..., c] = mask * color[c]\n",
    "\n",
    "        image = cv2.addWeighted(image, 1.0, colored_mask, 0.5, 0)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 720, 1280)\n",
      "float32\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1, 3, 736, 1280])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 이미지 전처리\n",
    "img = cv2.imread(\"frame_0167.jpg\")\n",
    "img = cv2.resize(img, (1280, 720))  # (width, height)\n",
    "img = img.astype(np.float32) / 255.0\n",
    "img = img.transpose(2, 0, 1)[None]  # shape: (1, 3, 720, 1280)\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "print(type(img))\n",
    "padded_img = F.pad(torch.Tensor(img), pad=(0, 0, 8, 8), mode='constant', value=114)  # YOLOv8 default padding color: 114\n",
    "print(padded_img.shape)  # torch.Size([1, 3, 736, 1280])\n",
    "# ONNX 추론\n",
    "session = ort.InferenceSession(\"./resources/models/ver1_test.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "outputs = session.run(None, {\"images\": padded_img.numpy()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58676d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\kdt\\AppData\\Local\\Temp\\ipykernel_23240\\3995588234.py\", line 1, in <module>\n",
      "    from ultralytics import YOLO\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\__init__.py\", line 5, in <module>\n",
      "    from ultralytics.data.explorer.explorer import Explorer\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\data\\__init__.py\", line 3, in <module>\n",
      "    from .base import BaseDataset\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\data\\base.py\", line 19, in <module>\n",
      "    from .utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\data\\utils.py\", line 19, in <module>\n",
      "    from ultralytics.nn.autobackend import check_class_names\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 53, in <module>\n",
      "    class AutoBackend(nn.Module):\n",
      "  File \"c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 85, in AutoBackend\n",
      "    device=torch.device(\"cpu\"),\n",
      "c:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:85: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device=torch.device(\"cpu\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.3  Python-3.10.16 torch-2.3.1+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3260209 parameters, 0 gradients\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Segment' object has no attribute 'detect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./resources/models/yolov8_seg_onnx_ver1.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# model.export(\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     format='onnx',\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     imgsz=640,          # 학습 시 사용한 이미지 크기\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     simplify=True      # Simplify 비활성화 (일단 문제 최소화를 위해)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 학습 시 사용한 이미지 크기 (고정)\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 적절한 opset 버전 (다른 버전도 시도 가능)\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# 동적 차원 비활성화 (배치 크기 1, 고정된 imgsz)\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# FP16 양자화 비활성화\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mint8\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# INT8 양자화 비활성화\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimplify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Simplify 비활성화\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_dfnq.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 파일명 지정 (선택 사항)\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Bias 초기화\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델의 마지막 레이어 구조를 확인합니다:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\engine\\model.py:601\u001b[0m, in \u001b[0;36mModel.export\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m custom \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[0;32m    600\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\engine\\exporter.py:240\u001b[0m, in \u001b[0;36mExporter.__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 240\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# dry runs\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;129;01mand\u001b[39;00m onnx \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    242\u001b[0m     im, model \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mhalf(), model\u001b[38;5;241m.\u001b[39mhalf()  \u001b[38;5;66;03m# to FP16\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:112\u001b[0m, in \u001b[0;36mSegment.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    109\u001b[0m bs \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# batch size\u001b[39;00m\n\u001b[0;32m    111\u001b[0m mc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv4[i](x[i])\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl)], \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# mask coefficients\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, mc, p\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\yolov8_onnx\\lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Segment' object has no attribute 'detect'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# 학습한 segmentation 모델 로드\n",
    "model = YOLO(\"./resources/models/yolov8_seg_onnx_ver1.pt\")\n",
    "# model.export(\n",
    "#     format='onnx',\n",
    "#     imgsz=640,          # 학습 시 사용한 이미지 크기\n",
    "#     opset=18,           # 일반적으로 잘 호환되는 opset 버전 (다른 버전도 시도 가능)\n",
    "#     half=False,         # FP16 양자화 비활성화\n",
    "#     int8=False,         # INT8 양자화 비활성화\n",
    "#     simplify=True      # Simplify 비활성화 (일단 문제 최소화를 위해)\n",
    "# )\n",
    "\n",
    "model.export(\n",
    "    format='onnx',\n",
    "    imgsz=640,          # 학습 시 사용한 이미지 크기 (고정)\n",
    "    opset=15,           # 적절한 opset 버전 (다른 버전도 시도 가능)\n",
    "    dynamic=False,      # 동적 차원 비활성화 (배치 크기 1, 고정된 imgsz)\n",
    "    half=False,         # FP16 양자화 비활성화\n",
    "    int8=False,         # INT8 양자화 비활성화\n",
    "    simplify=False,     # Simplify 비활성화\n",
    "    name='test_dfnq.onnx' # 파일명 지정 (선택 사항)\n",
    ")\n",
    "\n",
    "# Bias 초기화\n",
    "\n",
    "print(\"모델의 마지막 레이어 구조를 확인합니다:\")\n",
    "print(model.model.model[-1]) # 이 출력을 보고 bias를 가진 정확한 레이어를 찾아야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21514b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델의 마지막 레이어 파라미터 상태 ---\n",
      "Layer: cv2.0.0.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.3418, Max: 0.3103, Mean: -0.0042\n",
      "Layer: cv2.0.0.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.7769, Max: 1.2090, Mean: 0.9979\n",
      "Layer: cv2.0.0.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.1996, Max: 0.1220, Mean: -0.0609\n",
      "Layer: cv2.0.1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -1.3672, Max: 1.8018, Mean: -0.0113\n",
      "Layer: cv2.0.1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.9663, Max: 1.3359, Mean: 1.1104\n",
      "Layer: cv2.0.1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.0446, Max: 0.1648, Mean: 0.0531\n",
      "Layer: cv2.0.2.weight, Shape: torch.Size([64, 64, 1, 1])\n",
      "  Min: -0.4336, Max: 0.4558, Mean: -0.0244\n",
      "Layer: cv2.0.2.bias, Shape: torch.Size([64])\n",
      "  Min: -1.1084, Max: 2.5781, Mean: 0.9851\n",
      "Layer: cv2.1.0.conv.weight, Shape: torch.Size([64, 128, 3, 3])\n",
      "  Min: -0.5576, Max: 0.5210, Mean: -0.0019\n",
      "Layer: cv2.1.0.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.7007, Max: 1.4375, Mean: 0.9885\n",
      "Layer: cv2.1.0.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.1365, Max: 0.2440, Mean: 0.0044\n",
      "Layer: cv2.1.1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.7207, Max: 1.3438, Mean: -0.0064\n",
      "Layer: cv2.1.1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.6597, Max: 1.4453, Mean: 0.9607\n",
      "Layer: cv2.1.1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.0654, Max: 0.1941, Mean: 0.0246\n",
      "Layer: cv2.1.2.weight, Shape: torch.Size([64, 64, 1, 1])\n",
      "  Min: -0.8633, Max: 0.8750, Mean: -0.0245\n",
      "Layer: cv2.1.2.bias, Shape: torch.Size([64])\n",
      "  Min: -0.9390, Max: 2.4355, Mean: 0.9909\n",
      "Layer: cv2.2.0.conv.weight, Shape: torch.Size([64, 256, 3, 3])\n",
      "  Min: -0.4065, Max: 0.3533, Mean: -0.0012\n",
      "Layer: cv2.2.0.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.8442, Max: 1.4482, Mean: 1.0656\n",
      "Layer: cv2.2.0.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.1654, Max: 0.4023, Mean: 0.0294\n",
      "Layer: cv2.2.1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.8892, Max: 0.9097, Mean: -0.0065\n",
      "Layer: cv2.2.1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.5132, Max: 1.4404, Mean: 0.8929\n",
      "Layer: cv2.2.1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.1316, Max: 0.1981, Mean: 0.0161\n",
      "Layer: cv2.2.2.weight, Shape: torch.Size([64, 64, 1, 1])\n",
      "  Min: -1.5430, Max: 1.3438, Mean: -0.0205\n",
      "Layer: cv2.2.2.bias, Shape: torch.Size([64])\n",
      "  Min: -1.1748, Max: 2.8340, Mean: 0.9872\n",
      "Layer: cv3.0.0.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.2666, Max: 0.3098, Mean: -0.0033\n",
      "Layer: cv3.0.0.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.8765, Max: 1.2061, Mean: 1.0037\n",
      "Layer: cv3.0.0.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.2484, Max: 0.1621, Mean: -0.0579\n",
      "Layer: cv3.0.1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.1506, Max: 0.2627, Mean: -0.0041\n",
      "Layer: cv3.0.1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.8760, Max: 1.4639, Mean: 1.2954\n",
      "Layer: cv3.0.1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: 0.0472, Max: 0.4226, Mean: 0.3384\n",
      "Layer: cv3.0.2.weight, Shape: torch.Size([11, 64, 1, 1])\n",
      "  Min: -0.5029, Max: 0.3572, Mean: -0.2377\n",
      "Layer: cv3.0.2.bias, Shape: torch.Size([11])\n",
      "  Min: -9.9297, Max: -9.7656, Mean: -9.8842\n",
      "Layer: cv3.1.0.conv.weight, Shape: torch.Size([64, 128, 3, 3])\n",
      "  Min: -0.5605, Max: 0.6909, Mean: -0.0016\n",
      "Layer: cv3.1.0.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.6416, Max: 1.1309, Mean: 1.0000\n",
      "Layer: cv3.1.0.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.2542, Max: 0.1719, Mean: -0.0808\n",
      "Layer: cv3.1.1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.2729, Max: 0.3850, Mean: -0.0040\n",
      "Layer: cv3.1.1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.7930, Max: 1.9316, Mean: 1.2810\n",
      "Layer: cv3.1.1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.0371, Max: 0.5288, Mean: 0.3399\n",
      "Layer: cv3.1.2.weight, Shape: torch.Size([11, 64, 1, 1])\n",
      "  Min: -0.8340, Max: 0.2822, Mean: -0.2352\n",
      "Layer: cv3.1.2.bias, Shape: torch.Size([11])\n",
      "  Min: -8.5547, Max: -8.1641, Mean: -8.4808\n",
      "Layer: cv3.2.0.conv.weight, Shape: torch.Size([64, 256, 3, 3])\n",
      "  Min: -0.5142, Max: 0.6387, Mean: -0.0008\n",
      "Layer: cv3.2.0.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.8965, Max: 1.3301, Mean: 1.0241\n",
      "Layer: cv3.2.0.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.3901, Max: 0.1216, Mean: -0.1052\n",
      "Layer: cv3.2.1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.3232, Max: 0.4324, Mean: -0.0036\n",
      "Layer: cv3.2.1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.9385, Max: 2.4238, Mean: 1.3615\n",
      "Layer: cv3.2.1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.0879, Max: 0.6353, Mean: 0.3574\n",
      "Layer: cv3.2.2.weight, Shape: torch.Size([11, 64, 1, 1])\n",
      "  Min: -1.1113, Max: 0.2008, Mean: -0.2229\n",
      "Layer: cv3.2.2.bias, Shape: torch.Size([11])\n",
      "  Min: -7.1641, Max: -6.8164, Mean: -7.0852\n",
      "Layer: dfl.conv.weight, Shape: torch.Size([1, 16, 1, 1])\n",
      "  Min: 0.0000, Max: 15.0000, Mean: 7.5000\n",
      "Layer: proto.cv1.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.5049, Max: 0.4307, Mean: -0.0042\n",
      "Layer: proto.cv1.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.7954, Max: 1.3574, Mean: 1.0106\n",
      "Layer: proto.cv1.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.5137, Max: 0.3076, Mean: -0.1035\n",
      "Layer: proto.upsample.weight, Shape: torch.Size([64, 64, 2, 2])\n",
      "  Min: -0.2366, Max: 0.2571, Mean: 0.0012\n",
      "Layer: proto.upsample.bias, Shape: torch.Size([64])\n",
      "  Min: -0.6313, Max: 0.5625, Mean: -0.0195\n",
      "Layer: proto.cv2.conv.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "  Min: -0.4194, Max: 0.3389, Mean: -0.0017\n",
      "Layer: proto.cv2.bn.weight, Shape: torch.Size([64])\n",
      "  Min: 0.7690, Max: 1.2939, Mean: 1.0488\n",
      "Layer: proto.cv2.bn.bias, Shape: torch.Size([64])\n",
      "  Min: -0.4880, Max: 0.2539, Mean: -0.0470\n",
      "Layer: proto.cv3.conv.weight, Shape: torch.Size([32, 64, 1, 1])\n",
      "  Min: -1.5322, Max: 1.7451, Mean: -0.0054\n",
      "Layer: proto.cv3.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.6240, Max: 1.3828, Mean: 0.8586\n",
      "Layer: proto.cv3.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.4539, Max: 0.3884, Mean: -0.1005\n",
      "Layer: cv4.0.0.conv.weight, Shape: torch.Size([32, 64, 3, 3])\n",
      "  Min: -0.2627, Max: 0.2380, Mean: -0.0029\n",
      "Layer: cv4.0.0.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.8877, Max: 1.0908, Mean: 1.0028\n",
      "Layer: cv4.0.0.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.1602, Max: 0.0870, Mean: -0.0016\n",
      "Layer: cv4.0.1.conv.weight, Shape: torch.Size([32, 32, 3, 3])\n",
      "  Min: -0.2761, Max: 0.1652, Mean: -0.0089\n",
      "Layer: cv4.0.1.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.7705, Max: 1.0693, Mean: 0.9439\n",
      "Layer: cv4.0.1.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.1488, Max: 0.0538, Mean: -0.0288\n",
      "Layer: cv4.0.2.weight, Shape: torch.Size([32, 32, 1, 1])\n",
      "  Min: -0.5522, Max: 0.3228, Mean: -0.0035\n",
      "Layer: cv4.0.2.bias, Shape: torch.Size([32])\n",
      "  Min: -1.0059, Max: 1.1123, Mean: -0.0712\n",
      "Layer: cv4.1.0.conv.weight, Shape: torch.Size([32, 128, 3, 3])\n",
      "  Min: -0.4126, Max: 0.2947, Mean: -0.0022\n",
      "Layer: cv4.1.0.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.9390, Max: 1.1611, Mean: 1.0258\n",
      "Layer: cv4.1.0.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.2019, Max: 0.1488, Mean: -0.0127\n",
      "Layer: cv4.1.1.conv.weight, Shape: torch.Size([32, 32, 3, 3])\n",
      "  Min: -0.3755, Max: 0.2808, Mean: -0.0107\n",
      "Layer: cv4.1.1.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.7104, Max: 1.1465, Mean: 0.9146\n",
      "Layer: cv4.1.1.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.1858, Max: 0.0880, Mean: -0.0302\n",
      "Layer: cv4.1.2.weight, Shape: torch.Size([32, 32, 1, 1])\n",
      "  Min: -0.7285, Max: 0.3638, Mean: -0.0035\n",
      "Layer: cv4.1.2.bias, Shape: torch.Size([32])\n",
      "  Min: -0.9136, Max: 0.9702, Mean: -0.1022\n",
      "Layer: cv4.2.0.conv.weight, Shape: torch.Size([32, 256, 3, 3])\n",
      "  Min: -0.2522, Max: 0.2002, Mean: -0.0009\n",
      "Layer: cv4.2.0.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.9502, Max: 1.3359, Mean: 1.0674\n",
      "Layer: cv4.2.0.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.1288, Max: 0.1769, Mean: 0.0048\n",
      "Layer: cv4.2.1.conv.weight, Shape: torch.Size([32, 32, 3, 3])\n",
      "  Min: -0.4172, Max: 0.2166, Mean: -0.0118\n",
      "Layer: cv4.2.1.bn.weight, Shape: torch.Size([32])\n",
      "  Min: 0.7148, Max: 1.0830, Mean: 0.8869\n",
      "Layer: cv4.2.1.bn.bias, Shape: torch.Size([32])\n",
      "  Min: -0.1554, Max: 0.1400, Mean: -0.0298\n",
      "Layer: cv4.2.2.weight, Shape: torch.Size([32, 32, 1, 1])\n",
      "  Min: -0.6729, Max: 0.3516, Mean: -0.0077\n",
      "Layer: cv4.2.2.bias, Shape: torch.Size([32])\n",
      "  Min: -0.8193, Max: 0.8076, Mean: -0.0835\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model = YOLO(\"./resources/models/yolov8_seg_onnx_ver1.pt\")\n",
    "\n",
    "# 모델의 마지막 레이어 (Segment 헤드) 접근\n",
    "head = model.model.model[-1]\n",
    "\n",
    "print(\"--- 모델의 마지막 레이어 파라미터 상태 ---\")\n",
    "for name, param in head.named_parameters():\n",
    "    if 'weight' in name or 'bias' in name:\n",
    "        if param.numel() > 0: # 파라미터가 비어있지 않은 경우에만\n",
    "            print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "            print(f\"  Min: {param.min().item():.4f}, Max: {param.max().item():.4f}, Mean: {param.mean().item():.4f}\")\n",
    "            # 만약 모든 값이 0.0000에 가깝다면 문제가 심각한 것입니다.\n",
    "        else:\n",
    "            print(f\"Layer: {name}, Shape: {param.shape} (Empty parameter)\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5d3b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.145  Python-3.9.21 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,404,320 parameters, 0 gradients, 12.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n-seg.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 116, 8400), (1, 32, 160, 160)) (6.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.6s, saved as 'yolov8n-seg.onnx' (13.2 MB)\n",
      "\n",
      "Export complete (0.9s)\n",
      "Results saved to \u001b[1mC:\\Users\\kdt\\OneDrive\\ \\PROJECT_MNV\\App\u001b[0m\n",
      "Predict:         yolo predict task=segment model=yolov8n-seg.onnx imgsz=640  \n",
      "Validate:        yolo val task=segment model=yolov8n-seg.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8n-seg.onnx'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# yolov8n-seg.pt 다운로드 및 로드\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "# ONNX로 변환 (모델 이름 변경)\n",
    "model.export(format='onnx', imgsz=640, simplify=False, opset=11, name='yolov8n_seg_official.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2b625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.145  Python-3.9.21 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'resources\\models\\yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 20...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.5s, saved as 'resources\\models\\yolov8n.onnx' (12.2 MB)\n",
      "\n",
      "Export complete (0.7s)\n",
      "Results saved to \u001b[1mC:\\Users\\kdt\\OneDrive\\ \\PROJECT_MNV\\App\\resources\\models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=resources\\models\\yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=resources\\models\\yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'resources\\\\models\\\\yolov8n.onnx'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# 학습한 segmentation 모델 로드\n",
    "# model = YOLO(\"./resources/models/yolov8_seg_onnx_ver1.pt\")\n",
    "model = YOLO(\"./resources/models/yolov8n.pt\")\n",
    "\n",
    "# ✅ ONNX export\n",
    "model.export(format='onnx', imgsz=640, simplify=False, opset=20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014c49fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from best.onnx failed:Load model best.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ONNX 세션 생성\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCPUExecutionProvider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m input_name \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m      9\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# [1, 3, 640, 640] 예시\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\DL_TORCH\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32mc:\\Users\\kdt\\anaconda3\\envs\\DL_TORCH\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:480\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[1;32m--> 480\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[1;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from best.onnx failed:Load model best.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ONNX 세션 생성\n",
    "session = ort.InferenceSession(\"best.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape  # [1, 3, 640, 640] 예시\n",
    "\n",
    "# 이미지 불러오기 및 전처리\n",
    "img = cv2.imread(\"sample.jpg\")\n",
    "img_resized = cv2.resize(img, (640, 640))\n",
    "img_input = img_resized.transpose(2, 0, 1)  # HWC → CHW\n",
    "img_input = np.expand_dims(img_input, axis=0).astype(np.float32) / 255.0\n",
    "\n",
    "# ONNX 추론 실행\n",
    "outputs = session.run(None, {input_name: img_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb56ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def process_output(boxes_data, mask_protos, img_shape, conf_thres=0.25, iou_thres=0.5):\n",
    "    boxes_data = torch.tensor(boxes_data[0])  # shape: (N, 4 + 1 + nc + 32)\n",
    "    mask_protos = torch.tensor(mask_protos[0])  # shape: (32, 160, 160)\n",
    "\n",
    "    # 1. confidence + class 추출\n",
    "    box_xywh = boxes_data[:, 0:4]\n",
    "    objectness = boxes_data[:, 4:5]\n",
    "    class_probs = boxes_data[:, 5:5+1]  # binary segmentation: class1만 있다고 가정\n",
    "    mask_coeffs = boxes_data[:, 5+1:]  # shape: (N, 32)\n",
    "\n",
    "    conf = objectness * class_probs\n",
    "    keep = conf.view(-1) > conf_thres\n",
    "    boxes_data = boxes_data[keep]\n",
    "    conf = conf[keep]\n",
    "    mask_coeffs = mask_coeffs[keep]\n",
    "    box_xywh = box_xywh[keep]\n",
    "\n",
    "    # 2. xywh → xyxy\n",
    "    boxes_xyxy = box_xywh.clone()\n",
    "    boxes_xyxy[:, 0] = box_xywh[:, 0] - box_xywh[:, 2] / 2\n",
    "    boxes_xyxy[:, 1] = box_xywh[:, 1] - box_xywh[:, 3] / 2\n",
    "    boxes_xyxy[:, 2] = box_xywh[:, 0] + box_xywh[:, 2] / 2\n",
    "    boxes_xyxy[:, 3] = box_xywh[:, 1] + box_xywh[:, 3] / 2\n",
    "\n",
    "    # 3. NMS 적용\n",
    "    keep_idx = torchvision.ops.nms(boxes_xyxy, conf.view(-1), iou_thres)\n",
    "    boxes_xyxy = boxes_xyxy[keep_idx]\n",
    "    conf = conf[keep_idx]\n",
    "    mask_coeffs = mask_coeffs[keep_idx]\n",
    "\n",
    "    # 4. 마스크 복원\n",
    "    masks = torch.einsum(\"nc,chw->nhw\", mask_coeffs, mask_protos)\n",
    "    masks = sigmoid(masks)\n",
    "    masks = torch.nn.functional.interpolate(masks.unsqueeze(1), size=img_shape[:2], mode='bilinear', align_corners=False)\n",
    "    masks = masks.squeeze(1)\n",
    "    masks = masks > 0.5  # binarize\n",
    "\n",
    "    return boxes_xyxy.numpy(), conf.numpy(), masks.numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
