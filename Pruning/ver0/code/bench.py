"""
í”„ë£¨ë‹ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (í•µì‹¬ ê¸°ëŠ¥ë§Œ)
ì›ë³¸ vs í”„ë£¨ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥, ì†ë„, í¬ê¸° ë¹„êµ
"""

import torch
import time
import numpy as np
from ultralytics import YOLO
import psutil
import os

class PruningBenchmark:
    def __init__(self, original_model_path, pruned_model_path):
        """
        Args:
            original_model_path: ì›ë³¸ ëª¨ë¸ ê²½ë¡œ
            pruned_model_path: í”„ë£¨ë‹ëœ ëª¨ë¸ ê²½ë¡œ
        """
        print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.original_model = YOLO(original_model_path)
        self.pruned_model = YOLO(pruned_model_path)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    def get_model_info(self, model, label):
        """ëª¨ë¸ ì •ë³´ ì¶”ì¶œ"""
        # íŒŒë¼ë¯¸í„° ìˆ˜
        total_params = sum(p.numel() for p in model.model.parameters())
        
        # ëª¨ë¸ í¬ê¸° (MB)
        param_size = sum(p.numel() * p.element_size() for p in model.model.parameters())
        buffer_size = sum(b.numel() * b.element_size() for b in model.model.buffers())
        size_mb = (param_size + buffer_size) / (1024 * 1024)
        
        return {
            'label': label,
            'params': total_params,
            'size_mb': size_mb
        }
    
    def benchmark_speed(self, model, num_runs=100, img_size=(192, 320)):
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        model.model.eval()
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        dummy_input = torch.randn(1, 3, img_size[0], img_size[1])
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.model.to(device)
        dummy_input = dummy_input.to(device)
        
        # ì›Œë°ì—… (5íšŒ)
        with torch.no_grad():
            for _ in range(5):
                _ = model.model(dummy_input)
        
        # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.perf_counter()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model.model(dummy_input)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.perf_counter()
        
        # ê²°ê³¼ ê³„ì‚°
        total_time = end_time - start_time
        avg_time_ms = (total_time / num_runs) * 1000
        fps = 1000 / avg_time_ms
        
        return {
            'avg_time_ms': avg_time_ms,
            'fps': fps,
            'total_time': total_time
        }
    
    def benchmark_memory(self, model):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            # ë”ë¯¸ ì¶”ë¡ 
            dummy_input = torch.randn(1, 3, 192, 320).cuda()
            model.model.cuda()
            
            with torch.no_grad():
                _ = model.model(dummy_input)
            
            memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)
            torch.cuda.empty_cache()
            
            return {'gpu_memory_mb': memory_mb}
        else:
            # CPU ë©”ëª¨ë¦¬ (ëŒ€ëµì )
            process = psutil.Process()
            memory_mb = process.memory_info().rss / (1024 * 1024)
            return {'cpu_memory_mb': memory_mb}
    
    def validate_accuracy(self, data_path=None):
        """ì •í™•ë„ ê²€ì¦ (ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ìš°)"""
        if not data_path or not os.path.exists(data_path):
            print("âš ï¸  ê²€ì¦ ë°ì´í„°ì…‹ì´ ì—†ì–´ ì •í™•ë„ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None, None
        
        print("ğŸ“Š ì •í™•ë„ ê²€ì¦ ì¤‘...")
        
        # ë°ì´í„° íƒ€ì… í™•ì¸ (YAML íŒŒì¼ vs í´ë”)
        if os.path.isfile(data_path) and data_path.endswith('.yaml'):
            # YAML íŒŒì¼ì¸ ê²½ìš°
            data_source = data_path
            print(f"   ğŸ“ YAML ë°ì´í„°ì…‹ ì‚¬ìš©: {data_path}")
        elif os.path.isdir(data_path):
            # í´ë”ì¸ ê²½ìš°
            data_source = data_path
            print(f"   ğŸ“ í´ë” ë°ì´í„°ì…‹ ì‚¬ìš©: {data_path}")
        else:
            print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {data_path}")
            return None, None
        
        try:
            # ì›ë³¸ ëª¨ë¸ ê²€ì¦
            print("   ğŸ”„ ì›ë³¸ ëª¨ë¸ ê²€ì¦ ì¤‘...")
            original_results = self.original_model.val(data=data_source, verbose=False)
            
            # í”„ë£¨ë‹ ëª¨ë¸ ê²€ì¦  
            print("   ğŸ”„ í”„ë£¨ë‹ ëª¨ë¸ ê²€ì¦ ì¤‘...")
            pruned_results = self.pruned_model.val(data=data_source, verbose=False)
            
            return {
                'original_map50': float(original_results.box.map50),
                'original_map': float(original_results.box.map),
                'pruned_map50': float(pruned_results.box.map50), 
                'pruned_map': float(pruned_results.box.map)
            }, None
            
        except Exception as e:
            print(f"âš ï¸  ì •í™•ë„ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None
    
    def run_benchmark(self, data_path=None, num_runs=100):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("\nğŸš€ í”„ë£¨ë‹ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 50)
        
        # 1. ëª¨ë¸ ì •ë³´ ë¹„êµ
        print("\n1ï¸âƒ£ ëª¨ë¸ ì •ë³´:")
        original_info = self.get_model_info(self.original_model, "ì›ë³¸")
        pruned_info = self.get_model_info(self.pruned_model, "í”„ë£¨ë‹")
        
        print(f"ğŸ“Š {original_info['label']} ëª¨ë¸:")
        print(f"   íŒŒë¼ë¯¸í„°: {original_info['params']:,}ê°œ")
        print(f"   í¬ê¸°: {original_info['size_mb']:.2f} MB")
        
        print(f"ğŸ“Š {pruned_info['label']} ëª¨ë¸:")
        print(f"   íŒŒë¼ë¯¸í„°: {pruned_info['params']:,}ê°œ")
        print(f"   í¬ê¸°: {pruned_info['size_mb']:.2f} MB")
        
        # ê°œì„ ìœ¨ ê³„ì‚°
        param_reduction = (1 - pruned_info['params'] / original_info['params']) * 100
        size_reduction = (1 - pruned_info['size_mb'] / original_info['size_mb']) * 100
        
        print(f"\nğŸ“‰ ê°ì†Œìœ¨:")
        print(f"   íŒŒë¼ë¯¸í„°: {param_reduction:.1f}% ê°ì†Œ")
        print(f"   ëª¨ë¸ í¬ê¸°: {size_reduction:.1f}% ê°ì†Œ")
        
        # 2. ì†ë„ ë²¤ì¹˜ë§ˆí¬
        print(f"\n2ï¸âƒ£ ì†ë„ ë²¤ì¹˜ë§ˆí¬ ({num_runs}íšŒ í‰ê· ):")
        
        print("â±ï¸  ì›ë³¸ ëª¨ë¸ ì†ë„ ì¸¡ì • ì¤‘...")
        original_speed = self.benchmark_speed(self.original_model, num_runs)
        
        print("â±ï¸  í”„ë£¨ë‹ ëª¨ë¸ ì†ë„ ì¸¡ì • ì¤‘...")
        pruned_speed = self.benchmark_speed(self.pruned_model, num_runs)
        
        print(f"ğŸ”„ ì›ë³¸ ëª¨ë¸:")
        print(f"   ì¶”ë¡  ì‹œê°„: {original_speed['avg_time_ms']:.2f}ms")
        print(f"   FPS: {original_speed['fps']:.1f}")
        
        print(f"ğŸ”„ í”„ë£¨ë‹ ëª¨ë¸:")
        print(f"   ì¶”ë¡  ì‹œê°„: {pruned_speed['avg_time_ms']:.2f}ms") 
        print(f"   FPS: {pruned_speed['fps']:.1f}")
        
        # ì†ë„ í–¥ìƒ ê³„ì‚°
        speed_improvement = pruned_speed['fps'] / original_speed['fps']
        time_reduction = (1 - pruned_speed['avg_time_ms'] / original_speed['avg_time_ms']) * 100
        
        print(f"\nâš¡ ì†ë„ í–¥ìƒ:")
        print(f"   FPS í–¥ìƒ: {speed_improvement:.2f}ë°°")
        print(f"   ì‹œê°„ ë‹¨ì¶•: {time_reduction:.1f}%")
        
        # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        print(f"\n3ï¸âƒ£ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        
        original_memory = self.benchmark_memory(self.original_model)
        pruned_memory = self.benchmark_memory(self.pruned_model)
        
        memory_key = 'gpu_memory_mb' if torch.cuda.is_available() else 'cpu_memory_mb'
        memory_type = 'GPU' if torch.cuda.is_available() else 'CPU'
        
        print(f"ğŸ’¾ ì›ë³¸ ëª¨ë¸ ({memory_type}): {original_memory[memory_key]:.1f} MB")
        print(f"ğŸ’¾ í”„ë£¨ë‹ ëª¨ë¸ ({memory_type}): {pruned_memory[memory_key]:.1f} MB")
        
        memory_reduction = (1 - pruned_memory[memory_key] / original_memory[memory_key]) * 100
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%")
        
        # 4. ì •í™•ë„ ë¹„êµ (ì˜µì…˜)
        if data_path:
            accuracy_results, _ = self.validate_accuracy(data_path)
            if accuracy_results:
                print(f"\n4ï¸âƒ£ ì •í™•ë„ ë¹„êµ:")
                print(f"ğŸ“ˆ ì›ë³¸ ëª¨ë¸:")
                print(f"   mAP50: {accuracy_results['original_map50']:.3f}")
                print(f"   mAP50-95: {accuracy_results['original_map']:.3f}")
                
                print(f"ğŸ“ˆ í”„ë£¨ë‹ ëª¨ë¸:")
                print(f"   mAP50: {accuracy_results['pruned_map50']:.3f}")
                print(f"   mAP50-95: {accuracy_results['pruned_map']:.3f}")
                
                # ì •í™•ë„ ë³€í™”
                map50_change = accuracy_results['pruned_map50'] - accuracy_results['original_map50']
                map_change = accuracy_results['pruned_map'] - accuracy_results['original_map']
                
                print(f"ğŸ“Š ì •í™•ë„ ë³€í™”:")
                print(f"   mAP50: {map50_change:+.3f}")
                print(f"   mAP50-95: {map_change:+.3f}")
        
        # 5. ì¢…í•© ìš”ì•½
        print(f"\nğŸ¯ ì¢…í•© ìš”ì•½:")
        print("=" * 30)
        print(f"âš¡ ì†ë„ í–¥ìƒ: {speed_improvement:.2f}ë°°")
        print(f"ğŸ“‰ íŒŒë¼ë¯¸í„° ê°ì†Œ: {param_reduction:.1f}%")
        print(f"ğŸ’¾ í¬ê¸° ê°ì†Œ: {size_reduction:.1f}%")
        print(f"ğŸ§  ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%")
        
        # íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚°
        efficiency_score = (speed_improvement * (param_reduction/100) * (size_reduction/100)) * 100
        print(f"ğŸ† íš¨ìœ¨ì„± ì ìˆ˜: {efficiency_score:.1f}/100")
        
        print(f"\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


def main():
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    ORIGINAL_MODEL = r"C:\Users\KDT34\Desktop\Group6\original_model.pt"  # ì›ë³¸ ëª¨ë¸
    PRUNED_MODEL = r"C:\Users\KDT34\Desktop\Group6\pruning_model.pt"  # í”„ë£¨ë‹ ëª¨ë¸
    
    # ê²€ì¦ ë°ì´í„° ì„¤ì • - ë³€í™˜ëœ YOLO í˜•ì‹ í´ë” ì‚¬ìš©
    DATA_FOLDER = r"C:\Users\KDT34\Desktop\Group6\temp_validation"  # ë³€í™˜ëœ YOLO ë°ì´í„°ì…‹
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = PruningBenchmark(ORIGINAL_MODEL, PRUNED_MODEL)
    
    # ë³€í™˜ëœ YOLO ë°ì´í„°ì…‹ìœ¼ë¡œ ì •í™•ë„ ë¹„êµ
    benchmark.run_benchmark(data_path=DATA_FOLDER, num_runs=50)


if __name__ == "__main__":
    main()