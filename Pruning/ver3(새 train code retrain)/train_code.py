"""
í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ ì™„ì „íŒ ì½”ë“œ
- ìë™ YAML ìƒì„± (JSON+ì´ë¯¸ì§€ â†’ train/val ë¶„í• )
- í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥
- ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”

ì‚¬ìš©ë²•:
1. í”„ë£¨ë‹ëœ ëª¨ë¸ íŒŒì¼ (.pt)
2. JSON+ì´ë¯¸ì§€ê°€ ìˆëŠ” ë°ì´í„° í´ë”
3. classes.txt íŒŒì¼
"""

import os
import json
import torch
import time
import shutil
import tempfile
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from pathlib import Path
from PIL import Image
import random
from tqdm import tqdm

class CompletePrunedModelRetrainer:
    def __init__(self, pruned_model_path, data_dir, classes_file):
        """
        Args:
            pruned_model_path: í”„ë£¨ë‹ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pt)
            data_dir: JSON+ì´ë¯¸ì§€ê°€ ìˆëŠ” ë°ì´í„° í´ë”
            classes_file: classes.txt íŒŒì¼ ê²½ë¡œ
        """
        self.pruned_model_path = pruned_model_path
        self.data_dir = data_dir
        self.classes_file = classes_file
        self.model = None
        self.results = None
        self.temp_dataset_dir = None
        self.dataset_yaml_path = None
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pruned_model_path):
            raise FileNotFoundError(f"í”„ë£¨ë‹ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pruned_model_path}")
        
        if not os.path.exists(data_dir):
            raise FileNotFoundError(f"ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
            
        if not os.path.exists(classes_file):
            raise FileNotFoundError(f"í´ë˜ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {classes_file}")
        
        # í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ
        with open(classes_file, 'r') as f:
            self.classes = [line.strip() for line in f.readlines()]
        
        print(f"ğŸš€ í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ ì™„ì „íŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"   í”„ë£¨ë‹ëœ ëª¨ë¸: {pruned_model_path}")
        print(f"   ë°ì´í„° í´ë”: {data_dir}")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {len(self.classes)}")
        
        # matplotlib ì„¤ì •
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
    
    def prepare_dataset(self, train_ratio=0.8, max_samples=None):
        """JSON+ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  train/val ë¶„í• """
        print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_dataset_dir = tempfile.mkdtemp(prefix="retrain_dataset_")
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        train_images_dir = os.path.join(self.temp_dataset_dir, "images", "train")
        train_labels_dir = os.path.join(self.temp_dataset_dir, "labels", "train")
        val_images_dir = os.path.join(self.temp_dataset_dir, "images", "val")
        val_labels_dir = os.path.join(self.temp_dataset_dir, "labels", "val")
        
        for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # ëª¨ë“  í•˜ìœ„ í´ë”ì—ì„œ JSONê³¼ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        print(f"   ë©”ì¸ ë°ì´í„° í´ë” ìŠ¤ìº”: {self.data_dir}")
        
        all_paired_files = []
        subfolders = [f for f in os.listdir(self.data_dir) 
                     if os.path.isdir(os.path.join(self.data_dir, f))]
        
        print(f"   ë°œê²¬ëœ í•˜ìœ„ í´ë”: {len(subfolders)}ê°œ")
        
        for subfolder in subfolders:
            subfolder_path = os.path.join(self.data_dir, subfolder)
            print(f"   ìŠ¤ìº” ì¤‘: {subfolder}")
            
            try:
                # ê° í•˜ìœ„ í´ë”ì—ì„œ íŒŒì¼ ì°¾ê¸°
                files_in_subfolder = os.listdir(subfolder_path)
                json_files = [f for f in files_in_subfolder if f.endswith('.json')]
                image_files = [f for f in files_in_subfolder if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                
                # íŒŒì¼ ìŒ ë§¤ì¹­
                subfolder_pairs = []
                for json_file in json_files:
                    base_name = os.path.splitext(json_file)[0]
                    for img_file in image_files:
                        if base_name == os.path.splitext(img_file)[0]:
                            # ì „ì²´ ê²½ë¡œ ì €ì¥
                            json_path = os.path.join(subfolder_path, json_file)
                            img_path = os.path.join(subfolder_path, img_file)
                            subfolder_pairs.append((json_path, img_path, f"{subfolder}_{base_name}"))
                            break
                
                all_paired_files.extend(subfolder_pairs)
                print(f"     ì°¾ì€ íŒŒì¼ ìŒ: {len(subfolder_pairs)}ê°œ")
                
            except Exception as e:
                print(f"     âš ï¸ í´ë” ìŠ¤ìº” ì˜¤ë¥˜: {e}")
                continue
        
        paired_files = all_paired_files
        
        print(f"\nğŸ“Š ì „ì²´ ìŠ¤ìº” ê²°ê³¼:")
        print(f"   ì´ íŒŒì¼ ìŒ: {len(paired_files)}ê°œ")
        print(f"   ìŠ¤ìº”í•œ í´ë”: {len(subfolders)}ê°œ")
        
        if len(paired_files) == 0:
            print("âŒ ë§¤ì¹­ë˜ëŠ” JSON+ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if max_samples and len(paired_files) > max_samples:
            random.shuffle(paired_files)
            paired_files = paired_files[:max_samples]
            print(f"   ì‚¬ìš©í•  ìƒ˜í”Œ: {len(paired_files)}ê°œ (ì œí•œë¨)")
        
        # train/val ë¶„í• 
        random.shuffle(paired_files)
        train_size = int(len(paired_files) * train_ratio)
        train_pairs = paired_files[:train_size]
        val_pairs = paired_files[train_size:]
        
        print(f"   Train: {len(train_pairs)}ê°œ, Val: {len(val_pairs)}ê°œ")
        
        # ë°ì´í„° ë³€í™˜ ë° ë³µì‚¬
        def process_pairs(pairs, img_dir, label_dir, desc):
            for json_path, img_path, unique_name in tqdm(pairs, desc=desc):
                # ì´ë¯¸ì§€ ë³µì‚¬ (ê³ ìœ í•œ ì´ë¦„ ì‚¬ìš©)
                new_img_path = os.path.join(img_dir, f"{unique_name}.jpg")
                shutil.copy(img_path, new_img_path)
                
                # JSON â†’ YOLO ë³€í™˜
                yolo_annotations = self._convert_json_to_yolo(json_path, img_path)
                
                # ë¼ë²¨ íŒŒì¼ ì €ì¥
                label_path = os.path.join(label_dir, f"{unique_name}.txt")
                with open(label_path, 'w') as f:
                    f.write('\n'.join(yolo_annotations))
        
        process_pairs(train_pairs, train_images_dir, train_labels_dir, "Train ë°ì´í„° ì²˜ë¦¬")
        process_pairs(val_pairs, val_images_dir, val_labels_dir, "Val ë°ì´í„° ì²˜ë¦¬")
        
        # YAML íŒŒì¼ ìƒì„±
        self.dataset_yaml_path = os.path.join(self.temp_dataset_dir, "dataset.yaml")
        self._create_dataset_yaml()
        
        print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {self.temp_dataset_dir}")
        return True
    
    def _convert_json_to_yolo(self, json_path, img_path):
        """JSON ë¼ë²¨ì„ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        img = Image.open(img_path)
        img_width, img_height = img.size
        
        # JSON íŒŒì‹±
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        yolo_annotations = []
        shapes = data.get("shapes", [])
        
        for shape in shapes:
            label = shape.get("label", "")
            points = shape.get("points", [])
            shape_type = shape.get("shape_type", "")
            
            if label in self.classes and (shape_type == "polygon" or shape_type == "rectangle"):
                class_id = self.classes.index(label)
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                x_coords = [p[0] for p in points]
                y_coords = [p[1] for p in points]
                
                x_min, x_max = min(x_coords), max(x_coords)
                y_min, y_max = min(y_coords), max(y_coords)
                
                # YOLO í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
                x_center = (x_min + x_max) / (2 * img_width)
                y_center = (y_min + y_max) / (2 * img_height)
                width = (x_max - x_min) / img_width
                height = (y_max - y_min) / img_height
                
                yolo_annotation = f"{class_id} {x_center} {y_center} {width} {height}"
                yolo_annotations.append(yolo_annotation)
        
        return yolo_annotations
    
    def _create_dataset_yaml(self):
        """YOLO ë°ì´í„°ì…‹ YAML íŒŒì¼ ìƒì„±"""
        yaml_content = f"""path: {self.temp_dataset_dir}
train: images/train
val: images/val

nc: {len(self.classes)}
names: {self.classes}
"""
        with open(self.dataset_yaml_path, 'w') as f:
            f.write(yaml_content)
        
        print(f"   YAML íŒŒì¼ ìƒì„±: {self.dataset_yaml_path}")
    
    def load_pruned_model(self):
        """í”„ë£¨ë‹ëœ ëª¨ë¸ ë¡œë“œ ë° ì •ë³´ í™•ì¸"""
        print(f"\nğŸ“‚ í”„ë£¨ë‹ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        try:
            self.model = YOLO(self.pruned_model_path)
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            total_params = sum(p.numel() for p in self.model.model.parameters())
            file_size_mb = os.path.getsize(self.pruned_model_path) / (1024 * 1024)
            
            print(f"âœ… í”„ë£¨ë‹ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}ê°œ")
            print(f"   ëª¨ë¸ í¬ê¸°: {file_size_mb:.2f} MB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def benchmark_speed(self, label="", num_runs=20):
        """ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        if label:
            print(f"\nâš¡ {label} ì†ë„ ì¸¡ì •...")
        
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì¸¡ì •
        dummy_image = np.random.randint(0, 255, (180, 320, 3), dtype=np.uint8)
        
        # ì›Œë°ì—…
        for _ in range(3):
            _ = self.model.predict(dummy_image, verbose=False)
        
        # ì‹¤ì œ ì¸¡ì •
        start_time = time.perf_counter()
        for _ in range(num_runs):
            _ = self.model.predict(dummy_image, verbose=False)
        end_time = time.perf_counter()
        
        avg_time = (end_time - start_time) / num_runs * 1000  # ms
        fps = 1000 / avg_time
        
        print(f"   ì¶”ë¡  ì†ë„: {avg_time:.2f}ms ({fps:.1f} FPS)")
        return {'avg_time_ms': avg_time, 'fps': fps}
    
    def retrain_model(self, epochs=30, batch_size=16, learning_rate=0.001, patience=15):
        """í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ"""
        print(f"\nğŸ”„ í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")
        print(f"   ì—í­: {epochs}, ë°°ì¹˜: {batch_size}, í•™ìŠµë¥ : {learning_rate}")
        
        if self.model is None or self.dataset_yaml_path is None:
            print("âŒ ëª¨ë¸ì´ë‚˜ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ì¬í•™ìŠµ ì„¤ì •
            train_args = {
                'data': self.dataset_yaml_path,
                'epochs': epochs,
                'batch': batch_size,
                'imgsz': [180, 320],
                
                # í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ ìµœì í™” ì„¤ì •
                'lr0': learning_rate,
                'lrf': learning_rate * 0.01,
                'patience': patience,
                'save_period': max(5, epochs//6),
                
                # ì•ˆì •ì„± ì„¤ì •
                'warmup_epochs': 3,
                'cos_lr': True,
                'weight_decay': 0.0005,
                
                # í™˜ê²½ ì„¤ì •
                'device': 0 if torch.cuda.is_available() else 'cpu',
                'workers': 4,
                'verbose': True,
                'val': True,
                'plots': True,
                
                # êµ¬ì¡° ë³´ì¡´ ì„¤ì •
                'freeze': 0,
                'dropout': 0.0,
                'close_mosaic': 10,
            }
            
            print(f"ğŸ”¥ ì¬í•™ìŠµ ì‹œì‘...")
            start_time = time.time()
            
            # ì¬í•™ìŠµ ì‹¤í–‰
            self.results = self.model.train(**train_args)
            
            end_time = time.time()
            training_time = end_time - start_time
            
            print(f"âœ… ì¬í•™ìŠµ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time/60:.1f}ë¶„)")
            return True
            
        except Exception as e:
            print(f"âŒ ì¬í•™ìŠµ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def evaluate_model(self, label=""):
        """ëª¨ë¸ í‰ê°€"""
        if label:
            print(f"\nğŸ“Š {label} í‰ê°€...")
        
        if self.model is None or self.dataset_yaml_path is None:
            return None
        
        try:
            val_results = self.model.val(
                data=self.dataset_yaml_path,
                imgsz=[180, 320],
                verbose=True
            )
            
            # ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics = {
                'mAP50': float(val_results.box.map50),
                'mAP50-95': float(val_results.box.map),
                'Precision': float(val_results.box.p.mean()) if hasattr(val_results.box.p, 'mean') else float(val_results.box.p),
                'Recall': float(val_results.box.r.mean()) if hasattr(val_results.box.r, 'mean') else float(val_results.box.r),
            }
            
            if label:
                print(f"ğŸ“ˆ {label} ì„±ëŠ¥:")
                for metric_name, value in metrics.items():
                    print(f"   {metric_name}: {value:.4f}")
            
            return metrics
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            return None
    
    def create_training_charts(self, save_dir="./"):
        """í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ìƒì„±"""
        print(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        if self.results is None:
            print("âŒ í•™ìŠµ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # YOLO í•™ìŠµ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (results.csv íŒŒì¼ ì‚¬ìš©)
            results_dir = None
            
            # runs í´ë”ì—ì„œ ìµœì‹  ì‹¤í—˜ ê²°ê³¼ ì°¾ê¸°
            runs_dir = "runs/detect"
            if os.path.exists(runs_dir):
                train_dirs = [d for d in os.listdir(runs_dir) if d.startswith('train')]
                if train_dirs:
                    latest_dir = max(train_dirs, key=lambda x: os.path.getctime(os.path.join(runs_dir, x)))
                    results_dir = os.path.join(runs_dir, latest_dir)
            
            csv_file = None
            if results_dir and os.path.exists(os.path.join(results_dir, "results.csv")):
                csv_file = os.path.join(results_dir, "results.csv")
            
            if csv_file:
                self._create_charts_from_csv(csv_file, save_dir)
            else:
                self._create_basic_charts(save_dir)
            
        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def _create_charts_from_csv(self, csv_file, save_dir):
        """CSV íŒŒì¼ì—ì„œ í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„±"""
        import pandas as pd
        
        # CSV ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_file)
        df.columns = df.columns.str.strip()  # ê³µë°± ì œê±°
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        epochs = df['epoch'] if 'epoch' in df.columns else range(len(df))
        
        # 1. Loss ê³¡ì„ 
        if 'train/box_loss' in df.columns:
            ax1.plot(epochs, df['train/box_loss'], label='Train Box Loss', color='blue')
        if 'val/box_loss' in df.columns:
            ax1.plot(epochs, df['val/box_loss'], label='Val Box Loss', color='red')
        ax1.set_title('Box Loss Curves', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. mAP ê³¡ì„ 
        if 'metrics/mAP50(B)' in df.columns:
            ax2.plot(epochs, df['metrics/mAP50(B)'], label='mAP50', color='green', linewidth=2)
        if 'metrics/mAP50-95(B)' in df.columns:
            ax2.plot(epochs, df['metrics/mAP50-95(B)'], label='mAP50-95', color='orange', linewidth=2)
        ax2.set_title('mAP Curves', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('mAP')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Precision/Recall
        if 'metrics/precision(B)' in df.columns:
            ax3.plot(epochs, df['metrics/precision(B)'], label='Precision', color='purple')
        if 'metrics/recall(B)' in df.columns:
            ax3.plot(epochs, df['metrics/recall(B)'], label='Recall', color='brown')
        ax3.set_title('Precision & Recall Curves', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Score')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Learning Rate
        if 'lr/pg0' in df.columns:
            ax4.plot(epochs, df['lr/pg0'], label='Learning Rate', color='red')
            ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Learning Rate')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        chart_path = os.path.join(save_dir, 'training_curves.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… í•™ìŠµ ê³¡ì„  ì €ì¥: {chart_path}")
    
    def _create_basic_charts(self, save_dir):
        """ê¸°ë³¸ ì°¨íŠ¸ ìƒì„± (CSVê°€ ì—†ëŠ” ê²½ìš°)"""
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        ax.text(0.5, 0.5, 'Training Completed\nDetailed charts available in runs/detect/train/', 
                ha='center', va='center', fontsize=16, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.5))
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('Retraining Results', fontsize=18, fontweight='bold')
        
        chart_path = os.path.join(save_dir, 'training_completed.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ê¸°ë³¸ ì°¨íŠ¸ ì €ì¥: {chart_path}")
    
    def create_comparison_charts(self, original_metrics, original_speed, 
                               retrained_metrics, retrained_speed, save_dir="./"):
        """ì¬í•™ìŠµ ì „í›„ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„± ì¤‘...")
        
        try:
            # 1. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ë©”íŠ¸ë¦­ ë¹„êµ
            if original_metrics and retrained_metrics:
                metrics = list(original_metrics.keys())
                original_values = list(original_metrics.values())
                retrained_values = list(retrained_metrics.values())
                
                x = np.arange(len(metrics))
                width = 0.35
                
                ax1.bar(x - width/2, original_values, width, label='Original (Pruned)', 
                       alpha=0.8, color='skyblue')
                ax1.bar(x + width/2, retrained_values, width, label='Retrained', 
                       alpha=0.8, color='lightcoral')
                
                ax1.set_xlabel('Metrics')
                ax1.set_ylabel('Score')
                ax1.set_title('Performance Comparison', fontsize=14, fontweight='bold')
                ax1.set_xticks(x)
                ax1.set_xticklabels(metrics)
                ax1.legend()
                ax1.grid(True, alpha=0.3)
                
                # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
                for i, (orig, retrain) in enumerate(zip(original_values, retrained_values)):
                    ax1.text(i - width/2, orig + 0.01, f'{orig:.3f}', 
                            ha='center', va='bottom', fontweight='bold')
                    ax1.text(i + width/2, retrain + 0.01, f'{retrain:.3f}', 
                            ha='center', va='bottom', fontweight='bold')
            
            # ì†ë„ ë¹„êµ
            if original_speed and retrained_speed:
                speed_labels = ['FPS', 'Inference Time (ms)']
                original_speed_values = [original_speed['fps'], original_speed['avg_time_ms']]
                retrained_speed_values = [retrained_speed['fps'], retrained_speed['avg_time_ms']]
                
                x = np.arange(len(speed_labels))
                
                ax2.bar(x - width/2, original_speed_values, width, label='Original (Pruned)', 
                       alpha=0.8, color='lightgreen')
                ax2.bar(x + width/2, retrained_speed_values, width, label='Retrained', 
                       alpha=0.8, color='orange')
                
                ax2.set_xlabel('Speed Metrics')
                ax2.set_ylabel('Value')
                ax2.set_title('Speed Comparison', fontsize=14, fontweight='bold')
                ax2.set_xticks(x)
                ax2.set_xticklabels(speed_labels)
                ax2.legend()
                ax2.grid(True, alpha=0.3)
                
                # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
                for i, (orig, retrain) in enumerate(zip(original_speed_values, retrained_speed_values)):
                    ax2.text(i - width/2, orig + max(original_speed_values)*0.02, f'{orig:.1f}', 
                            ha='center', va='bottom', fontweight='bold')
                    ax2.text(i + width/2, retrain + max(retrained_speed_values)*0.02, f'{retrain:.1f}', 
                            ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            comparison_path = os.path.join(save_dir, 'performance_comparison.png')
            plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì €ì¥: {comparison_path}")
            
        except Exception as e:
            print(f"âŒ ë¹„êµ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def save_retrained_model(self, output_path=None):
        """ì¬í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        if output_path is None:
            base_name = Path(self.pruned_model_path).stem
            output_path = f"{base_name}_retrained.pt"
        
        print(f"\nğŸ’¾ ì¬í•™ìŠµëœ ëª¨ë¸ ì €ì¥...")
        
        try:
            self.model.save(output_path)
            file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
            return output_path
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        if self.temp_dataset_dir and os.path.exists(self.temp_dataset_dir):
            try:
                shutil.rmtree(self.temp_dataset_dir)
                print(f"ğŸ§¹ ì„ì‹œ ë°ì´í„°ì…‹ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def full_retrain_pipeline(self, epochs=30, batch_size=16, learning_rate=0.001, 
                            patience=15, output_path=None, save_charts=True):
        """ì „ì²´ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 70)
        print("ğŸš€ í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ ì™„ì „íŒ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 70)
        
        original_metrics = None
        original_speed = None
        retrained_metrics = None
        retrained_speed = None
        
        try:
            # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
            if not self.prepare_dataset(max_samples=None):  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 1000ìƒ˜í”Œë¡œ ì œí•œ
                return False
            
            # 2. í”„ë£¨ë‹ëœ ëª¨ë¸ ë¡œë“œ
            if not self.load_pruned_model():
                return False
            
            # 3. ì¬í•™ìŠµ ì „ ì„±ëŠ¥ ì¸¡ì •
            print("\n" + "="*50)
            print("ğŸ“Š ì¬í•™ìŠµ ì „ ì„±ëŠ¥ ì¸¡ì •")
            print("="*50)
            original_metrics = self.evaluate_model("ì¬í•™ìŠµ ì „")
            original_speed = self.benchmark_speed("ì¬í•™ìŠµ ì „")
            
            # 4. ì¬í•™ìŠµ ì‹¤í–‰
            print("\n" + "="*50)
            print("ğŸ”¥ ì¬í•™ìŠµ ì‹¤í–‰")
            print("="*50)
            if not self.retrain_model(epochs, batch_size, learning_rate, patience):
                return False
            
            # 5. ì¬í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì •
            print("\n" + "="*50)
            print("ğŸ“Š ì¬í•™ìŠµ í›„ ì„±ëŠ¥ ì¸¡ì •")
            print("="*50)
            retrained_metrics = self.evaluate_model("ì¬í•™ìŠµ í›„")
            retrained_speed = self.benchmark_speed("ì¬í•™ìŠµ í›„")
            
            # 6. ëª¨ë¸ ì €ì¥
            saved_path = self.save_retrained_model(output_path)
            
            # 7. ê·¸ë˜í”„ ìƒì„±
            if save_charts:
                print("\n" + "="*50)
                print("ğŸ“Š ê²°ê³¼ ê·¸ë˜í”„ ìƒì„±")
                print("="*50)
                self.create_training_charts()
                if original_metrics and retrained_metrics:
                    self.create_comparison_charts(original_metrics, original_speed,
                                                retrained_metrics, retrained_speed)
            
            # 8. ìµœì¢… ê²°ê³¼ ìš”ì•½
            print("\n" + "=" * 70)
            print("ğŸ“Š ì¬í•™ìŠµ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            print("=" * 70)
            
            if original_metrics and retrained_metrics:
                print(f"\nğŸ¯ ì„±ëŠ¥ ë¹„êµ:")
                print(f"{'ë©”íŠ¸ë¦­':<15} {'ì¬í•™ìŠµ ì „':<12} {'ì¬í•™ìŠµ í›„':<12} {'ê°œì„ ìœ¨':<10}")
                print("-" * 50)
                for metric in original_metrics.keys():
                    orig_val = original_metrics[metric]
                    new_val = retrained_metrics[metric]
                    improvement = ((new_val - orig_val) / orig_val * 100) if orig_val > 0 else 0
                    print(f"{metric:<15} {orig_val:<12.4f} {new_val:<12.4f} {improvement:+7.1f}%")
            
            if original_speed and retrained_speed:
                print(f"\nâš¡ ì†ë„ ë¹„êµ:")
                orig_fps = original_speed['fps']
                new_fps = retrained_speed['fps']
                fps_change = ((new_fps - orig_fps) / orig_fps * 100) if orig_fps > 0 else 0
                
                print(f"   ì¬í•™ìŠµ ì „: {orig_fps:.1f} FPS")
                print(f"   ì¬í•™ìŠµ í›„: {new_fps:.1f} FPS")
                print(f"   ì†ë„ ë³€í™”: {fps_change:+.1f}%")
            
            if saved_path:
                print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸: {saved_path}")
            
            if save_charts:
                print(f"\nğŸ“Š ìƒì„±ëœ ê·¸ë˜í”„:")
                print(f"   - training_curves.png (í•™ìŠµ ê³¡ì„ )")
                print(f"   - performance_comparison.png (ì„±ëŠ¥ ë¹„êµ)")
            
            print(f"\nâœ… ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            self.cleanup()


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ì„¤ì • - ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”
    PRUNED_MODEL_PATH = r"C:\Users\K\Desktop\Group_6\0610_ver3\pruned_model.pt"
    DATA_DIR = r"C:\Users\K\Desktop\Group_6\0610_ver3\data"  # ë©”ì¸ ë°ì´í„° í´ë” (í•˜ìœ„ í´ë”ë“¤ í¬í•¨)
    CLASSES_FILE = r"C:\Users\K\Desktop\Group_6\0602\data\classes.txt"
    
    # ì¬í•™ìŠµ ì„¤ì •
    EPOCHS = 30              # ì¬í•™ìŠµ ì—í­ ìˆ˜ (í”„ë£¨ë‹ëœ ëª¨ë¸ì€ ì ê²Œ)
    BATCH_SIZE = 150          # ë°°ì¹˜ í¬ê¸°
    LEARNING_RATE = 0.001    # ë‚®ì€ í•™ìŠµë¥  (ì•ˆì •ì  ì¬í•™ìŠµ)
    PATIENCE = 15            # Early stopping ì¸ë‚´
    OUTPUT_PATH = "retrained_pruned_model.pt"  # ì €ì¥í•  íŒŒì¼ëª…
    
    # ê²½ë¡œ í™•ì¸
    print("ğŸ” íŒŒì¼ ë° í´ë” í™•ì¸ ì¤‘...")
    if not os.path.exists(PRUNED_MODEL_PATH):
        print(f"âŒ í”„ë£¨ë‹ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PRUNED_MODEL_PATH}")
        return
    
    if not os.path.exists(DATA_DIR):
        print(f"âŒ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        return
    
    if not os.path.exists(CLASSES_FILE):
        print(f"âŒ í´ë˜ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CLASSES_FILE}")
        return
    
    # ë°ì´í„° í´ë” ë‚´ìš© í™•ì¸
    if os.path.exists(DATA_DIR):
        subfolders = [f for f in os.listdir(DATA_DIR) 
                     if os.path.isdir(os.path.join(DATA_DIR, f))]
        
        total_json = 0
        total_img = 0
        
        print(f"ğŸ“ í•˜ìœ„ í´ë” ìŠ¤ìº” ê²°ê³¼:")
        for subfolder in subfolders[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            subfolder_path = os.path.join(DATA_DIR, subfolder)
            try:
                files = os.listdir(subfolder_path)
                json_count = len([f for f in files if f.endswith('.json')])
                img_count = len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
                total_json += json_count
                total_img += img_count
                print(f"   {subfolder}: JSON {json_count}ê°œ, ì´ë¯¸ì§€ {img_count}ê°œ")
            except:
                continue
        
        if len(subfolders) > 5:
            print(f"   ... ë° ê¸°íƒ€ {len(subfolders)-5}ê°œ í´ë”")
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„ (ì˜ˆìƒ):")
        print(f"   í•˜ìœ„ í´ë”: {len(subfolders)}ê°œ")
        print(f"   ì´ JSON íŒŒì¼: ~{total_json}ê°œ ì´ìƒ")
        print(f"   ì´ ì´ë¯¸ì§€ íŒŒì¼: ~{total_img}ê°œ ì´ìƒ")
    else:
        print(f"âŒ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        return
    
    try:
        # ì¬í•™ìŠµê¸° ì´ˆê¸°í™”
        retrainer = CompletePrunedModelRetrainer(
            PRUNED_MODEL_PATH, 
            DATA_DIR, 
            CLASSES_FILE
        )
        
        # ì‚¬ìš©ì í™•ì¸
        user_input = input(f"\nì¬í•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n) [y]: ").strip().lower()
        if user_input == 'n':
            print("ì¬í•™ìŠµì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success = retrainer.full_retrain_pipeline(
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE,
            patience=PATIENCE,
            output_path=OUTPUT_PATH,
            save_charts=True
        )
        
        if success:
            print("\n" + "="*50)
            print("ğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("="*50)
            print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
            print(f"   - {OUTPUT_PATH} (ì¬í•™ìŠµëœ ëª¨ë¸)")
            print(f"   - training_curves.png (í•™ìŠµ ê³¡ì„ )")
            print(f"   - performance_comparison.png (ì„±ëŠ¥ ë¹„êµ)")
            print(f"   - runs/detect/train/ (ìƒì„¸ í•™ìŠµ ê²°ê³¼)")
        else:
            print("\nâŒ ì¬í•™ìŠµ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()